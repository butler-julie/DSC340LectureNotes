

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>A Conceptual and Mathematical Introduction to Neural Networks &#8212; Machine Learning and Neural Networks</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Week7_Slides_1';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Creating Neural Networks with Scikit-Learn and Keras" href="Week7_Slides_2.html" />
    <link rel="prev" title="Unsupervised Machine Learning: Clustering and Dimensionality Reduction" href="Week6_Slides.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    DSC 340: Machine Learning and Neural Networks
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Week_1_LectureNotes.html">Introduction to Machine Learning and the Mathematics of Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week2_LectureNotes.html">Linear Regression and the Machine Learning Workflow</a></li>

<li class="toctree-l1"><a class="reference internal" href="Week3_LectureNotes.html">Regularized Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week4_LectureNotes.html">Using Machine Learning to Solve Classification Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week5_LectureNotes.html">Model Optimization and Nonlinear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week6_LectureNotes.html">Unsupervised Machine Learning: Clustering and Dimensionality Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week7_LectureNotes_1.html">A Conceptual and Mathematical Introduction to Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week7_LectureNotes_2.html">Creating Neural Networks with Scikit-Learn and Keras</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week8_LectureNotes.html">Creating Neural Networks with Tensorflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week9_LectureNotes.html">Creating Neural Networks from Scratch</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Slides</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Week1_Slides.html">Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week2_Slides.html">Linear Regression and the Machine Learning Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week4_Slides.html">Classification Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week5_Slides.html">Model Optimization and Nonlinear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week6_Slides.html">Unsupervised Machine Learning: Clustering and Dimensionality Reduction</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">A Conceptual and Mathematical Introduction to Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week7_Slides_2.html">Creating Neural Networks with Scikit-Learn and Keras</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week8_Slides.html">Creating Neural Networks with Tensorflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week9_Slides.html">Creating Neural Networks from Scratch</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FWeek7_Slides_1.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Week7_Slides_1.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>A Conceptual and Mathematical Introduction to Neural Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-material-covered-so-far">Review of Material Covered So Far</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-machine-learning-workflow">The Machine Learning Workflow</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-set-iris-data-set">Data Set: Iris Data Set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">Ridge Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-ridge-regression">Kernel Ridge Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conceptual-neural-network">Conceptual Neural Network</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neuron-node">Neuron/Node</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layers">Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-layer">Input Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-layer">Output Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hidden-layers">Hidden Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fully-connected">Fully Connected</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feedforward">Feedforward</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">Loss Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-mathematics">Neural Network Mathematics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-neuron-weights-and-biases">Linear Neuron (Weights and Biases)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-function">Activation Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-for-first-hidden-layer">Mathematics for First Hidden Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-for-second-hidden-layer">Mathematics for Second Hidden Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-for-l-th-hidden-layer-or-output-layer">Mathematics for l-th Hidden Layer (Or Output Layer)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-for-loss-functions">Mathematics for Loss Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-loss-functions">Regression Loss Functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-loss-functions">Classification Loss Functions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-and-optimization">Backpropagation and Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#choice-of-optimizer">Choice of Optimizer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate">Learning Rate</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-hyperparameters">Neural Network Hyperparameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-and-activation-functions">Loss and Activation Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-neurons-and-layers">Architecture (Neurons and Layers)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-and-optimizer">Learning Rate and Optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#number-of-training-iterations">Number of Training Iterations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#others">Others</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-details">Other Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-process-and-iterations">Training Process and Iterations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-and-underfitting">Overfitting and Underfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-error-versus-test-error">Training Error Versus Test Error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vanishing-gradient">Vanishing Gradient</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inherent-randomness">Inherent Randomness</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="a-conceptual-and-mathematical-introduction-to-neural-networks">
<h1>A Conceptual and Mathematical Introduction to Neural Networks<a class="headerlink" href="#a-conceptual-and-mathematical-introduction-to-neural-networks" title="Permalink to this heading">#</a></h1>
<p>CSC/DSC 340 Week 7 Slides (Part 1)</p>
<p>Author: <span class="xref myst">Dr. Julie Butler</span></p>
<p>Date Created: September 26, 2023</p>
<p>Last Modified: September 26, 2023</p>
<section id="review-of-material-covered-so-far">
<h2>Review of Material Covered So Far<a class="headerlink" href="#review-of-material-covered-so-far" title="Permalink to this heading">#</a></h2>
<section id="the-machine-learning-workflow">
<h3>The Machine Learning Workflow<a class="headerlink" href="#the-machine-learning-workflow" title="Permalink to this heading">#</a></h3>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>1. Import and clean data; perform initial analysis
    * Same as with other machine learning models
    
2. Split the data into a training set and test set
     Same as with other machine learning models
     
3. Train your model
    * A slightly more complicated and lengthy process with neural networks
    
4. Test your model&#39;s performance with the test data
    * Same as with other machine learning models
    
5. Improve the model through further data refinement or hyperparameter tuning
    * Hyperparameter tuning will be much more complicated with neural networks
</pre></div>
</div>
</section>
<section id="data-set-iris-data-set">
<h3>Data Set: Iris Data Set<a class="headerlink" href="#data-set-iris-data-set" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Four features with three class labels (clasification problem)</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##############################</span>
<span class="c1">##          IMPORTS         ##</span>
<span class="c1">##############################</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the iris dataset from sklearn</span>
<span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>

<span class="c1"># Convert the iris dataset to a pandas dataframe</span>
<span class="n">iris_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>

<span class="c1"># Add the target variable to the dataframe</span>
<span class="n">iris_data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">iris_data</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s1">&#39;target&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/butlerju/Library/Python/3.9/lib/python/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight
  self._figure.tight_layout(*args, **kwargs)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.PairGrid at 0x13f18a1c0&gt;
</pre></div>
</div>
<img alt="_images/1a3070f162f027a78c17b903f2ebc1ce6395e1bbed352105d12724a4bcfd6de9.png" src="_images/1a3070f162f027a78c17b903f2ebc1ce6395e1bbed352105d12724a4bcfd6de9.png" />
</div>
</div>
</section>
<section id="linear-regression">
<h3>Linear Regression<a class="headerlink" href="#linear-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Output for Linear Regression:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\hat{y} = X\theta\)</span></p></li>
<li><p>For a four feature data set: <span class="math notranslate nohighlight">\(\hat{y} = X_1\theta_1 + X_2\theta_2 + X_3\theta_3 + X_4\theta_4\)</span></p></li>
<li><p>The output can only be a linear combination of the inputs.</p></li>
</ul>
</li>
<li><p>Loss Function</p>
<ul>
<li><p>Mean-Squared Error: <span class="math notranslate nohighlight">\(J(\theta) = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2\)</span></p></li>
<li><p>The smaller the value the closer the predictions are to the true results.</p></li>
</ul>
</li>
<li><p>Optimized Weights/Parameters</p>
<ul>
<li><p>Found by minimizing the loss function with respect to the weights</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_{Linear} = (X^TX)^{-1}X^Ty\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="ridge-regression">
<h3>Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Output for Ridge Regression:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\hat{y} = X\theta\)</span></p></li>
<li><p>For a four feature data set: <span class="math notranslate nohighlight">\(\hat{y} = X_1\theta_1 + X_2\theta_2 + X_3\theta_3 + X_4\theta_4\)</span></p></li>
<li><p>The output can only be a linear combination of the inputs.</p></li>
</ul>
</li>
<li><p>Loss Function</p>
<ul>
<li><p>Regularized Mean-Squared Error: <span class="math notranslate nohighlight">\(J(\theta) = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2 + \alpha\sum_{i=1}^N|\theta_i|^2\)</span></p></li>
<li><p>The smaller the value the closer the predictions are to the true results.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is a hyperparameter whose value determines how regularized the algorithm is</p></li>
</ul>
</li>
<li><p>Optimized Weights/Parameters</p>
<ul>
<li><p>Found by minimizing the loss function with respect to the weights</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_{Ridge} = (X^TX-\alpha\textbf{I})^{-1}X^Ty\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="kernel-ridge-regression">
<h3>Kernel Ridge Regression<a class="headerlink" href="#kernel-ridge-regression" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Output for Kernel Ridge Regression:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\hat{y} = \sum_{i=1}^m \theta_ik(x_i,x)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(x_i\)</span> represents the m training points used to find <span class="math notranslate nohighlight">\(\theta\)</span></p></li>
<li><p>k is the kernel function, adds nonlinearity to the system</p></li>
<li><p>For a data set with three training points: <span class="math notranslate nohighlight">\(\hat{y}(x) = \theta_1k(x_1,x) + \theta_2k(x_2,x) + \theta_3k(x_3,x)\)</span></p></li>
<li><p>Choice of kernel function is a hyperparameter, each kernel function adds some hyperparameters as well</p></li>
</ul>
</li>
<li><p>Loss Function</p>
<ul>
<li><p>Regularized Mean-Squared Error: <span class="math notranslate nohighlight">\(J(\theta) = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2 + \alpha\sum_{i=1}^N|\theta_i|^2\)</span></p></li>
<li><p>The smaller the value the closer the predictions are to the true results.</p></li>
<li><p><span class="math notranslate nohighlight">\(\alpha\)</span> is a hyperparameter whose value determines how regularized the algorithm is</p></li>
<li><p>Optimized Weights</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\theta_{KRR} = (K-\alpha\textbf{I})^{-1}y\)</span></p></li>
<li><p>K is a kernel matrix</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>All machine learning methods we have investigated so far have closed-form solutions for their weights/parameters.</p>
</section>
</section>
<section id="conceptual-neural-network">
<h2>Conceptual Neural Network<a class="headerlink" href="#conceptual-neural-network" title="Permalink to this heading">#</a></h2>
<section id="neuron-node">
<h3>Neuron/Node<a class="headerlink" href="#neuron-node" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Base unit of a neural network; neural networks are made of many connected nodes</p></li>
<li><p>A node is just a function that takes an input, does some math, and produces an output
<img alt="Node" src="_images/Node.png" /></p></li>
</ul>
</section>
<section id="layers">
<h3>Layers<a class="headerlink" href="#layers" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Many nodes that all receive the same inputs but are not connected to each other</p></li>
<li><p>The number of nodes in each layer varies and depends on the type of layer
<img alt="Layer" src="_images/Layer.png" /></p></li>
</ul>
</section>
<section id="input-layer">
<h3>Input Layer<a class="headerlink" href="#input-layer" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>First layer in a neural network</p></li>
<li><p>The number of nodes is the same as the number of features in the data set; each node is passed on of the features</p></li>
<li><p>Does not manipulate its inputs, simply passes them onto the next layer</p></li>
</ul>
<p><img alt="Input Layer" src="_images/Input_Layer.png" /></p>
</section>
<section id="output-layer">
<h3>Output Layer<a class="headerlink" href="#output-layer" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The last layer in the neural network, produces the output</p></li>
<li><p>The number of nodes is equal to the number of dimensions of the output; each node outputs one of the dimensions</p></li>
<li><p>Manipulates data using the mathematical formula described later in the notebook</p></li>
</ul>
<p><img alt="Output Layer" src="_images/Output_Layer.png" /></p>
</section>
<section id="hidden-layers">
<h3>Hidden Layers<a class="headerlink" href="#hidden-layers" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Hidden layers are neural network layers that exist between the input and output layers</p></li>
<li><p>They take an input, manipulate it using the mathematical form discussed later in the notebook, and then pass the results to the next layer in the neural network</p></li>
<li><p>The number of hidden layers and the number of neurons per hidden layer is not controlled by the data set and has to be set by the user</p></li>
<li><p>The hidden layers do not have to have the same number of neurons per layer</p></li>
</ul>
<p><img alt="Hidden Layers" src="_images/Hidden_Layers.png" /></p>
</section>
<section id="fully-connected">
<h3>Fully Connected<a class="headerlink" href="#fully-connected" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Fully connected means that each neuron is connected to every neuron in the next layer</p></li>
<li><p>All networks we will look at in this course are fully connected (but this is not necessary try of all neural network architectures)</p></li>
</ul>
</section>
<section id="feedforward">
<h3>Feedforward<a class="headerlink" href="#feedforward" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Feedforward means that data moves only from the input layer in the direction of the output layer, it is never fed backward</p></li>
<li><p>“Regular” neural networks and convolutional neural networks are feed forward, recurrent neural networks are not</p></li>
</ul>
</section>
<section id="loss-function">
<h3>Loss Function<a class="headerlink" href="#loss-function" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The loss function determines how accurate the predictions of the neural network are and are used to find the values of the weights</p></li>
<li><p>Unlike the previous machine learning algorithms, the loss function is not set to be the mean-squared error function; it can be any function that determines how well the neural network has performed</p></li>
</ul>
<p><img alt="Loss Function" src="_images/Loss_Function.png" /></p>
</section>
<section id="backpropagation">
<h3>Backpropagation<a class="headerlink" href="#backpropagation" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Backpropagation is the process of optimizing the weights of the neural network to get a better result the next time the neural network is run</p></li>
<li><p>The loss function is used to update the weights of the network (represented in the diagram by the arrows)</p></li>
<li><p>The feedforward-loss function-backpropagation process is one “training iteration”, neural networks are trained with hundreds or thousands of training iterations</p></li>
</ul>
<p><img alt="Backpropagation" src="_images/Backpropagation.png" /></p>
</section>
</section>
<section id="neural-network-mathematics">
<h2>Neural Network Mathematics<a class="headerlink" href="#neural-network-mathematics" title="Permalink to this heading">#</a></h2>
<section id="linear-neuron-weights-and-biases">
<h3>Linear Neuron (Weights and Biases)<a class="headerlink" href="#linear-neuron-weights-and-biases" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[z_i^l = \sum_{j=1}^M w_{ij}^lx_j + b_i^l\]</div>
<ul class="simple">
<li><p>X is the input to the neuron</p></li>
<li><p>w is the weights matrix which belongs to the entire layer, each neuron has a specific column of the weights matrix</p>
<ul>
<li><p>Same function as <span class="math notranslate nohighlight">\(\theta\)</span> in previous algorithms</p></li>
</ul>
</li>
<li><p>b is a bias vector that corresponds to the entire layer, each neuron has a specific valye of the bias vector</p>
<ul>
<li><p>Similar function to fitting the intercept in previous algorithms</p></li>
</ul>
</li>
<li><p>Same for as linear and ridge regression <span class="math notranslate nohighlight">\(\longrightarrow\)</span> therefore can only give us linear results</p></li>
<li><p>Applies to neurons in both the hidden layers and in the output layer</p></li>
</ul>
</section>
<section id="activation-function">
<h3>Activation Function<a class="headerlink" href="#activation-function" title="Permalink to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[z_i^l = f_l(\sum_{j=1}^M w_{ij}^lx_j + b_i^l)\]</div>
<ul class="simple">
<li><p>The activation function is applied to the entire output; gives the output some nonlinearity</p></li>
<li><p>Neurons in the same layer will have the same activation function, different layers can have different activation
functions</p></li>
<li><p>A layer does not have to have an activation function (common for output layer)</p></li>
<li><p>Common activation functions:</p>
<ul>
<li><p>Sigmoid: <span class="math notranslate nohighlight">\(f(x) = \frac{1}{1+e^{-x}}\)</span></p></li>
<li><p>Hyperbolic Tangent: <span class="math notranslate nohighlight">\(f(x) = tanh(x)\)</span></p></li>
<li><p>Rectified Linear Unit (ReLU): <span class="math notranslate nohighlight">\(f(x) = max(0,x)\)</span></p></li>
</ul>
</li>
</ul>
</section>
<section id="mathematics-for-first-hidden-layer">
<h3>Mathematics for First Hidden Layer<a class="headerlink" href="#mathematics-for-first-hidden-layer" title="Permalink to this heading">#</a></h3>
<p>If each neuron in the first hidden layer has the form discussed above, then we can write the output of the entire first layer using its activation function, its weight matrix, and its bias vector using the following form. Here, the input to the first hidden layer is the output from the input layer, or simply the input to the neural network (the features).</p>
<div class="math notranslate nohighlight">
\[\hat{y}_1 = f_1(W_1\textbf{x} + \textbf{b}_1)\]</div>
</section>
<section id="mathematics-for-second-hidden-layer">
<h3>Mathematics for Second Hidden Layer<a class="headerlink" href="#mathematics-for-second-hidden-layer" title="Permalink to this heading">#</a></h3>
<p>We can write the output of the second hidden layer in the same way as we wrote the output for the first hidden layer, remembering that its input will be the output from the first hidden layer.</p>
<div class="math notranslate nohighlight">
\[\hat{y}_2 = f_2(W_2\hat{y}_1 + \textbf{b}_2)\]</div>
<p>But since we know what the output of the first hidden layer is, we can replace <span class="math notranslate nohighlight">\(\hat{y}_1\)</span> with its value in terms of the activation function, the weights matrix, and the bias vector.</p>
<div class="math notranslate nohighlight">
\[\hat{y}_2 = f_2(W_2f^1(W_1\textbf(x) + \textbf{b}_1) + \textbf{b}_2)\]</div>
</section>
<section id="mathematics-for-l-th-hidden-layer-or-output-layer">
<h3>Mathematics for l-th Hidden Layer (Or Output Layer)<a class="headerlink" href="#mathematics-for-l-th-hidden-layer-or-output-layer" title="Permalink to this heading">#</a></h3>
<p>Using the pattern established in the previous two sections to find a general form for the output of the l-th hidden layer. This equation also applies to the output layer as its neurons perform the same manipulations as the neurons in the hidden layer.</p>
<div class="math notranslate nohighlight">
\[\hat{y}_l = f_l (W_l \hat{y}_{l-1} + \textbf{b}_l)\]</div>
<p>We can take the above equation and replace <span class="math notranslate nohighlight">\(\hat{y}_{l-1}\)</span> with its value in terms of its activation function, weights matrix, and bias vector.</p>
<div class="math notranslate nohighlight">
\[\hat{y}_l = f_l(W_lf_{l-1}(W_{l-1}\hat{y}_{l-2} + \textbf{b}_{l-1}) + \textbf{b}_l)\]</div>
<p>We can keep doing this until we reach the equation for the first hidden layer, whose input is the input for the neural network.</p>
<div class="math notranslate nohighlight">
\[\hat{y}_l = f_l(W_lf_{l-1}(W_{l-1}(\cdot \cdot \cdot f_1(W_1\textbf{x} + \textbf{b}_1) \cdot \cdot \cdot) + \textbf{b}_{l-1}) + \textbf{b}_l)\]</div>
<p>Thus, assuming that the l-th layer is the output layer of the neural network, we have written an equation that explains how a neural network gets from its input data to its output. Now all that is left to do is determine how it figure out what values of the weight matrices and bias vectors will give the most accurate results.</p>
</section>
<section id="mathematics-for-loss-functions">
<h3>Mathematics for Loss Functions<a class="headerlink" href="#mathematics-for-loss-functions" title="Permalink to this heading">#</a></h3>
<p>The loss function is what determines how closely the output of the neural network matches the true (or expected results). In the algorithms we have looked at previously, the loss function was always some form of the mean-squared error function, but in neural networks, the loss function can be any function that determines how good the results of the neural network are. Thus, for neural networks, the loss function is an additional hyperparameter that the user has to set.  There are many common loss functions that are used for both regression and classification problems and you can also create custom loss functions.</p>
<section id="regression-loss-functions">
<h4>Regression Loss Functions<a class="headerlink" href="#regression-loss-functions" title="Permalink to this heading">#</a></h4>
<p>The most common loss function to use with regression problems is the mean-squared error:
$<span class="math notranslate nohighlight">\(J(\theta) = \frac{1}{N}\sum_{i=1}^N (y_i-\hat{y}_i)^2,\)</span><span class="math notranslate nohighlight">\(
but the mean absolute error is also common:
\)</span><span class="math notranslate nohighlight">\(J(\theta) = \frac{1}{N}\sum_{i=1}^N|y_i-\hat{y}_i|\)</span>$</p>
</section>
<section id="classification-loss-functions">
<h4>Classification Loss Functions<a class="headerlink" href="#classification-loss-functions" title="Permalink to this heading">#</a></h4>
<p>For classification the loss functions are different, reflecting the probability the neural network got the classification correct. For a binary classification problem, the most common lost function will be the binary cross-entropy which is defined as:</p>
<div class="math notranslate nohighlight">
\[ J(\theta) = -\frac{1}{n}\sum_{i=1}^n(y_ilog(p_i)+(1-y_i)log(1-p_i)),\]</div>
<p>where <span class="math notranslate nohighlight">\(p_i\)</span> is the probability that the point belongs to the first category.</p>
<p>For multiclass classification, the analogous loss function is the categorical cross-entropy, which is defined as:</p>
<div class="math notranslate nohighlight">
\[J(\theta) = -\frac{1}{n}\sum{i=1}^N\sum_{j=1}^M y_{ij}log(p_{ij}),\]</div>
<p>where i is a sum over all points and j is a sum over all categories.</p>
<p>The probability a point belongs to each category can be extracted from most neural network implementations.</p>
</section>
</section>
<section id="backpropagation-and-optimization">
<h3>Backpropagation and Optimization<a class="headerlink" href="#backpropagation-and-optimization" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Backpropagation is the process of using the loss function results to update the weights and biases of the neural network</p></li>
<li><p>Updates the weights/biases of the output layer, then the last hidden layer, and back through the network until the first hidden layer</p></li>
<li><p>Many different optimizers are used here and neural network optimization is a popular research topic in the field of machine learning</p></li>
</ul>
<section id="choice-of-optimizer">
<h4>Choice of Optimizer<a class="headerlink" href="#choice-of-optimizer" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>The choice of optimizer becomes a hyperparameter for neural networks</p></li>
<li><p>The simplest optimizer is gradient descent:
$<span class="math notranslate nohighlight">\( \theta_i = \theta_i + l_r\frac{\partial J(\theta)}{\theta_i}\)</span>$</p></li>
<li><p><span class="math notranslate nohighlight">\(\theta_i\)</span> is either a weight or bias</p></li>
<li><p>The most common modern optimizer is called Adam and is much more complicated</p></li>
</ul>
</section>
<section id="learning-rate">
<h4>Learning Rate<a class="headerlink" href="#learning-rate" title="Permalink to this heading">#</a></h4>
<ul class="simple">
<li><p>A floating point number (usually less than 1) that controls how much the weights and biases will change with each iteration</p></li>
<li><p>Too large and the optimal values may be overshot and too small means there will be very little change between iterations and the training process will be extended</p></li>
<li><p>Analogous to the strength of the regularization (<span class="math notranslate nohighlight">\(\alpha\)</span>) in ridge regression and kernel ridge regression</p></li>
</ul>
</section>
</section>
</section>
<section id="neural-network-hyperparameters">
<h2>Neural Network Hyperparameters<a class="headerlink" href="#neural-network-hyperparameters" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Neural networks have an extensive number of hyperparameters that need to be set by the user before the algorithm is run and this makes using neural networks a complicated and time-consuming process.</p></li>
<li><p>Hyperparameter tuning here becomes even more important than in previous algorithms but is also much more time-consuming due to the increased number of parameters and choices for each parameter.</p></li>
</ul>
<section id="loss-and-activation-functions">
<h3>Loss and Activation Functions<a class="headerlink" href="#loss-and-activation-functions" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The choice of loss function and activation functions are both hyperparameters</p></li>
<li><p>Though the loss function is a hyperparameter it is usually not tuned (it can be though)</p></li>
<li><p>The activation function needs to be chosen per hidden layer and for the output layer and does not have to be the same function for the entire network</p></li>
<li><p>No activation function is also an option on a neural network layer and is common for the output layer for regression problems so the output is not constrained by the range of the</p></li>
</ul>
</section>
<section id="architecture-neurons-and-layers">
<h3>Architecture (Neurons and Layers)<a class="headerlink" href="#architecture-neurons-and-layers" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The architecture of a neural network refers to the number of hidden layers and the number of neurons per hidden layer</p>
<ul>
<li><p>Also known as the width and depth of the neural network</p></li>
</ul>
</li>
<li><p>The number of hidden layers and the number of neurons in each hidden layer are hyperparameters</p>
<ul>
<li><p>The number of neurons in each hidden layer does not have to be the same</p></li>
</ul>
</li>
<li><p>The more hidden layers a network has the more of a chance it has to process the data and extract patterns</p></li>
<li><p>The more neurons in a hidden layer corresponds to a larger weight matrix and more of a chance to create meaningful combinations of the data</p></li>
<li><p>BUT a larger network takes longer to train and eventually adding more layers and neurons can be detrimental to the performance</p></li>
</ul>
</section>
<section id="learning-rate-and-optimizer">
<h3>Learning Rate and Optimizer<a class="headerlink" href="#learning-rate-and-optimizer" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>Both the learning rate and the optimizer are hyperparameters that have to be set before the network is trained</p></li>
<li><p>Here Adam is the most common value for the optimizer but the learning rate could be tuned to increase performance</p></li>
</ul>
</section>
<section id="number-of-training-iterations">
<h3>Number of Training Iterations<a class="headerlink" href="#number-of-training-iterations" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>The number of feedforward-loss function-backpropagation cycles (training iterations) is also a hyperparameter and the number of iterations can lead to losses in the accuracy of the neural network if it’s too low or too high (discussed later in this notebook)</p></li>
</ul>
</section>
<section id="others">
<h3>Others<a class="headerlink" href="#others" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li><p>There are other hyperparameters you can investigate as well including “dropout”, “regularization”, “early stopping”, and others</p></li>
<li><p>You are unlikely to encounter tuning these in this course unless you are curious about using them to improve the performance of your models for the final project</p></li>
</ul>
</section>
</section>
<section id="other-details">
<h2>Other Details<a class="headerlink" href="#other-details" title="Permalink to this heading">#</a></h2>
<section id="training-process-and-iterations">
<h3>Training Process and Iterations<a class="headerlink" href="#training-process-and-iterations" title="Permalink to this heading">#</a></h3>
<ul class="simple">
<li></li>
</ul>
</section>
<section id="overfitting-and-underfitting">
<h3>Overfitting and Underfitting<a class="headerlink" href="#overfitting-and-underfitting" title="Permalink to this heading">#</a></h3>
</section>
<section id="training-error-versus-test-error">
<h3>Training Error Versus Test Error<a class="headerlink" href="#training-error-versus-test-error" title="Permalink to this heading">#</a></h3>
</section>
<section id="vanishing-gradient">
<h3>Vanishing Gradient<a class="headerlink" href="#vanishing-gradient" title="Permalink to this heading">#</a></h3>
</section>
<section id="inherent-randomness">
<h3>Inherent Randomness<a class="headerlink" href="#inherent-randomness" title="Permalink to this heading">#</a></h3>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="Week6_Slides.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Unsupervised Machine Learning: Clustering and Dimensionality Reduction</p>
      </div>
    </a>
    <a class="right-next"
       href="Week7_Slides_2.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Creating Neural Networks with Scikit-Learn and Keras</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#review-of-material-covered-so-far">Review of Material Covered So Far</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-machine-learning-workflow">The Machine Learning Workflow</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#data-set-iris-data-set">Data Set: Iris Data Set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-regression">Linear Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression">Ridge Regression</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-ridge-regression">Kernel Ridge Regression</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conceptual-neural-network">Conceptual Neural Network</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#neuron-node">Neuron/Node</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layers">Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#input-layer">Input Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#output-layer">Output Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hidden-layers">Hidden Layers</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#fully-connected">Fully Connected</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#feedforward">Feedforward</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-function">Loss Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-mathematics">Neural Network Mathematics</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-neuron-weights-and-biases">Linear Neuron (Weights and Biases)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-function">Activation Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-for-first-hidden-layer">Mathematics for First Hidden Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-for-second-hidden-layer">Mathematics for Second Hidden Layer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-for-l-th-hidden-layer-or-output-layer">Mathematics for l-th Hidden Layer (Or Output Layer)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-for-loss-functions">Mathematics for Loss Functions</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#regression-loss-functions">Regression Loss Functions</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#classification-loss-functions">Classification Loss Functions</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation-and-optimization">Backpropagation and Optimization</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#choice-of-optimizer">Choice of Optimizer</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate">Learning Rate</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-hyperparameters">Neural Network Hyperparameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#loss-and-activation-functions">Loss and Activation Functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#architecture-neurons-and-layers">Architecture (Neurons and Layers)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate-and-optimizer">Learning Rate and Optimizer</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#number-of-training-iterations">Number of Training Iterations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#others">Others</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-details">Other Details</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-process-and-iterations">Training Process and Iterations</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#overfitting-and-underfitting">Overfitting and Underfitting</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#training-error-versus-test-error">Training Error Versus Test Error</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#vanishing-gradient">Vanishing Gradient</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#inherent-randomness">Inherent Randomness</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr. Julie Butler
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>