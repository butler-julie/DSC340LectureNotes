

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Model Optimization and Nonlinear Models &#8212; Machine Learning and Neural Networks</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Week5_LectureNotes';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Unsupervised Machine Learning: Clustering and Dimensionality Reduction" href="Week6_LectureNotes.html" />
    <link rel="prev" title="Using Machine Learning to Solve Classification Problems" href="Week4_LectureNotes.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    DSC 340: Machine Learning and Neural Networks
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Lecture Notes</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Week_1_LectureNotes.html">Introduction to Machine Learning and the Mathematics of Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week2_LectureNotes.html">Linear Regression and the Machine Learning Workflow</a></li>

<li class="toctree-l1"><a class="reference internal" href="Week3_LectureNotes.html">Regularized Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week4_LectureNotes.html">Using Machine Learning to Solve Classification Problems</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Model Optimization and Nonlinear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week6_LectureNotes.html">Unsupervised Machine Learning: Clustering and Dimensionality Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week7_LectureNotes_1.html">A Conceptual and Mathematical Introduction to Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week7_LectureNotes_2.html">Creating Neural Networks with Scikit-Learn and Keras</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week8_LectureNotes.html">Creating Neural Networks with Tensorflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week9_LectureNotes.html">Creating Neural Networks from Scratch</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Slides</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Week1_Slides.html">Introduction to Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week2_Slides.html">Linear Regression and the Machine Learning Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week4_Slides.html">Classification Problems</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week5_Slides.html">Model Optimization and Nonlinear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week6_Slides.html">Unsupervised Machine Learning: Clustering and Dimensionality Reduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week7_Slides_1.html">A Conceptual and Mathematical Introduction to Neural Networks</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week7_Slides_2.html">Creating Neural Networks with Scikit-Learn and Keras</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week8_Slides.html">Creating Neural Networks with Tensorflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="Week9_Slides.html">Creating Neural Networks from Scratch</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FWeek5_LectureNotes.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/Week5_LectureNotes.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Model Optimization and Nonlinear Models</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-hyperparameter-tuning">Part 1: Hyperparameter Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-hyperparameter-tuning">Why do we need hyperparameter tuning?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#methods-for-hyperparameter-tuning">Methods for Hyperparameter Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-default-values">Using Default Values</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#for-loop-tuning">For Loop Tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gridsearchcv-scikit-learn">GridSearchCV (Scikit-Learn)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#randomizedsearchcv">RandomizedSearchCV</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-ridge-regression">Bayesian Ridge Regression</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-ii-feature-engineering">Part II: Feature Engineering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-non-linear-models">Part 3: Non-linear Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-ridge-regression">Kernel Ridge Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-ridge-regression-equations">Kernel Ridge Regression Equations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-tuning-with-many-hyperparameters">Hyperparameter Tuning with Many Hyperparameters</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="model-optimization-and-nonlinear-models">
<h1>Model Optimization and Nonlinear Models<a class="headerlink" href="#model-optimization-and-nonlinear-models" title="Permalink to this headline">#</a></h1>
<p>CSC/DSC 340 Week 5 Lecture Notes</p>
<p>Author: <span class="xref myst">Dr. Julie Butler</span></p>
<p>Date Created: August 20, 2023</p>
<p>Last Modified: August 20, 2023</p>
<section id="part-1-hyperparameter-tuning">
<h2>Part 1: Hyperparameter Tuning<a class="headerlink" href="#part-1-hyperparameter-tuning" title="Permalink to this headline">#</a></h2>
<section id="why-do-we-need-hyperparameter-tuning">
<h3>Why do we need hyperparameter tuning?<a class="headerlink" href="#why-do-we-need-hyperparameter-tuning" title="Permalink to this headline">#</a></h3>
<p>We have been covering basic hyperparameter tuning for the past two weeks, but now that we are covering model optimization in detail we will take a more thorough look at hyperparameter tuning.</p>
<p>Hyperparameters are variables in a model that a user has to set before the model can be trained.  However, the value the user chooses for these variables can affect the results of the model.</p>
<p>If we consider ridge regression, the hyperparameter <span class="math notranslate nohighlight">\(\alpha\)</span> controls how regularized the model is.  A very small value of <span class="math notranslate nohighlight">\(\alpha\)</span> results in an unregularized model (equivalent to linear regression), but a very large value of <span class="math notranslate nohighlight">\(\alpha\)</span> results in a highly regularized model that is usually a bad fit.  We can examine the effect of different <span class="math notranslate nohighlight">\(\alpha\)</span> values on a new data set from Sci-kit learn called the California housing data set, which predicts the price of a house (in units of $100k) given information about the house such as its age and it average number of rooms, as well as information about the surrounding are like population and median income.  First we need to import various libraries and the data set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##############################</span>
<span class="c1">##         IMPORTS          ##</span>
<span class="c1">##############################</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Lasso</span><span class="p">,</span> <span class="n">Ridge</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span> <span class="k">as</span> <span class="n">MSE</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span> <span class="k">as</span> <span class="n">score</span>
<span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">fetch_california_housing</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Print the features</span>
<span class="n">fetch_california_housing</span><span class="p">()</span><span class="o">.</span><span class="n">feature_names</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&#39;MedInc&#39;,
 &#39;HouseAge&#39;,
 &#39;AveRooms&#39;,
 &#39;AveBedrms&#39;,
 &#39;Population&#39;,
 &#39;AveOccup&#39;,
 &#39;Latitude&#39;,
 &#39;Longitude&#39;]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">##############################</span>
<span class="c1">##        IMPORT DATA       ##</span>
<span class="c1">##############################</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">return_X_y</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="c1">##############################</span>
<span class="c1">##        SCALE DATA        ##</span>
<span class="c1">##############################</span>
<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">##############################</span>
<span class="c1">##     TRAIN-TEST SPLIT     ##</span>
<span class="c1">##############################</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we can train the model with a view different values of <span class="math notranslate nohighlight">\(\alpha\)</span> and compare the results.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test_plot</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_plot</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_test</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;True&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="p">[</span><span class="mf">1e-15</span><span class="p">,</span> <span class="mf">1e-2</span><span class="p">,</span> <span class="mf">1e2</span><span class="p">,</span> <span class="mf">1e4</span><span class="p">,</span> <span class="mf">1e15</span><span class="p">]:</span>
    <span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

    <span class="n">err</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;ALPHA:&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_plot</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_pred</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="sa">r</span><span class="s1">&#39;Predicted $\alpha$=&#39;</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>ALPHA: 1e-15 MSE: 0.5385006856668789
ALPHA: 0.01 MSE: 0.5385007720052859
ALPHA: 100.0 MSE: 0.5399747069672424
ALPHA: 10000.0 MSE: 0.7424641564380989
ALPHA: 1000000000000000.0 MSE: 1.3498672699729928
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x12f5d6a00&gt;
</pre></div>
</div>
<img alt="_images/a28767e6cfe67f42ca902ca1e09c5474e90a93e1db3cc077fa060f2ed8b3ce74.png" src="_images/a28767e6cfe67f42ca902ca1e09c5474e90a93e1db3cc077fa060f2ed8b3ce74.png" />
</div>
</div>
<p>We can see that the value for <span class="math notranslate nohighlight">\(\alpha\)</span> does not make a huge difference at small values, but as <span class="math notranslate nohighlight">\(\alpha\)</span> starts to increase so does the error of the model. Figuring out this point where the error starts to increase is important so that we do not choose an <span class="math notranslate nohighlight">\(\alpha\)</span> value higher than that.  However, in other data sets, a high degree of regularization could be needed, so choosing a value of <span class="math notranslate nohighlight">\(\alpha\)</span> that is too low could also be detrimental to our model’s performance.</p>
<p>While performing extensive hyperparameter tuning on a linear model like the ones we have studied so far may seem like overkill, beginning this week we will start looking at more complicated nonlinear models.  These nonlinear models have several hyperparamters and they models will be more sensitive to small changes in these parameters, so not only does hyperparameter tuning become more important, it also becomes more time extensive.  For the rest of this section of the lecture notes we will be looking at five different ways that can be used to find the optimal hyperparameter value for the ridge regression algorithm and now these method can be transferred to the more complicated linear models we will be studying for the remainder of the course.</p>
</section>
<section id="methods-for-hyperparameter-tuning">
<h3>Methods for Hyperparameter Tuning<a class="headerlink" href="#methods-for-hyperparameter-tuning" title="Permalink to this headline">#</a></h3>
<section id="using-default-values">
<h4>Using Default Values<a class="headerlink" href="#using-default-values" title="Permalink to this headline">#</a></h4>
<p>All machine learning algorithms that are implemented in Scikit-Learn have default values for all of the hyperparameters. These are not neccessarily the best values, but using the defaults is the simplest method.</p>
<p>For ridge regression, the default value of <span class="math notranslate nohighlight">\(\alpha\)</span> is 1.0, which is a reasonably high level of regularization. Let’s determine the performance of the default values on our housing data set. However, the entire housing data set is over 20,000 points, so in order to save time we will reduce the data set to just the first 1,000 points to perform hyperparameter tuning. This subset of the data set we use for the hyperparameter tuning is called the <strong>validation data set</strong>, and is either a subset of the entire data set or a subset of the training data set depending on the implementation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Make the data set smaller (20k+ points in total)</span>
<span class="c1"># More points = more data to generate patterns BUT more run time</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 736 µs, sys: 225 µs, total: 961 µs
Wall time: 846 µs
</pre></div>
</div>
</div>
</div>
<p>Not only are we interested in the accuracy of each tuning method, we also want to look at the run times. If a method can produce a very accurate method, but it has very high run times, it may not be the best choice.  Run time is even more important as we move to more complicated models and data sets. The <code class="docutils literal notranslate"><span class="pre">%%time</span></code> statement has to be the first line of the cell to produce the run time of the code cell. In this notebook we will be comparing the run times from just running each cell once, but in a true comparison of run times you will want to run each relevant cell many times and report the average run time as well as the standard deviation.  This also goes for reporting any errors/accuracy scores that have an element of randomness in them (such as the train-test split).  We will want to run the code many times to get an average accuracy over several possible data splits.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">err</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>

<span class="n">X_test_plot</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_plot</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_test</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;True&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_plot</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_pred</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE: 0.29965000307452666
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x12f6b1400&gt;
</pre></div>
</div>
<img alt="_images/3b437ffc944d6781986462070bce082c55e61d09fa59f86e2b8e441667969c6a.png" src="_images/3b437ffc944d6781986462070bce082c55e61d09fa59f86e2b8e441667969c6a.png" />
</div>
</div>
<p>The advantages of using the default values for the hyperparameters are that its a very fast method and it does not require any modifications to the code or algorithm.  However, the default values for the hyperparameters may not be the best values and no test are done to check to see if there are better values. The next three hyperparameter tuning methods we will look at check many different values (or combinations of values) to find the optimal set of hyperparameters.</p>
</section>
<section id="for-loop-tuning">
<h4>For Loop Tuning<a class="headerlink" href="#for-loop-tuning" title="Permalink to this headline">#</a></h4>
<p>This is the the method we have been using for hyperparameter tuning so far, where we use a for loop to perform a brute force test over a given range of possible values to find the hyperparameter value that results in the lowest error (or highest accuracy score). If there is more than one hyperparameter, we can use nested for loops to check all possible combinations of hyperparameters in the ranges we are testing.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">best_err</span> <span class="o">=</span> <span class="mf">1e4</span>
<span class="n">best_alpha</span> <span class="o">=</span> <span class="kc">None</span>
<span class="k">for</span> <span class="n">alpha</span> <span class="ow">in</span> <span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">)</span>
    <span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    
    <span class="n">err</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
    
    <span class="k">if</span> <span class="n">err</span> <span class="o">&lt;</span> <span class="n">best_err</span><span class="p">:</span>
        <span class="n">best_err</span> <span class="o">=</span> <span class="n">err</span>
        <span class="n">best_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 638 ms, sys: 1.9 ms, total: 640 ms
Wall time: 296 ms
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">best_alpha</span><span class="p">)</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">err</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;CHOSEN ALPHA:&quot;</span><span class="p">,</span> <span class="n">best_alpha</span><span class="p">)</span>

<span class="n">X_test_plot</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_plot</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_test</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;True&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_plot</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_pred</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE: 0.2996487006208541
CHOSEN ALPHA: 0.9704808877380326
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x12fa56640&gt;
</pre></div>
</div>
<img alt="_images/5324cda247feb312286085b3ab1fabd28a52c539b07bf119555a30d2357acee6.png" src="_images/5324cda247feb312286085b3ab1fabd28a52c539b07bf119555a30d2357acee6.png" />
</div>
</div>
<p>The advantages of the for loop method are that it checks more than one value of the hyperparameter to find the best value (it will check as many different values as we specify), its a simple concept that is relatively easy to implement, and it has short run times (compared to the next two methods). However, it is a long piece of code that grows longer as we have more hyperparamters to check and it does not check all possible values of the best hyperparameters, just the ones that are passed.</p>
</section>
<section id="gridsearchcv-scikit-learn">
<h4>GridSearchCV (Scikit-Learn)<a class="headerlink" href="#gridsearchcv-scikit-learn" title="Permalink to this headline">#</a></h4>
<p>Scikit-Learn has several hyperparameter tuning implementations and we will be looking at two of them: <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">GridSearchCV</a> and <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html">RandomizedSearchCV</a>. GridSearchCV is a brute force algorithm that checks every single hyperparameter it is given to find the best value, or every possible combination of hyperparameters if it is given more than one hyperparameter to optimize. This is the same process as the for loop tuning discussed in the last section, but GridSearchCV gives more information that the for loop implementation but it also has longer run times.  The below code applies GridSearchCV to our data set and extracts the best value of <span class="math notranslate nohighlight">\(\alpha\)</span>.  However, GridSearchCV has many more features that can be look at in the documenation (linked above).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>

<span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">15</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5000</span><span class="p">)}</span>

<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>

<span class="n">grid_search</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">ridge</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span>\
                           <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">)</span>
<span class="n">grid_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;alpha&#39;: 0.13293052652247003}
CPU times: user 13.2 s, sys: 43.2 ms, total: 13.3 s
Wall time: 12.9 s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">])</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">err</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;CHOSEN ALPHA:&#39;</span><span class="p">,</span> <span class="n">grid_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">])</span>

<span class="n">X_test_plot</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_plot</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_test</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;True&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_plot</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_pred</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE: 0.3014494621759448
CHOSEN ALPHA: 0.13293052652247003
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x12f779460&gt;
</pre></div>
</div>
<img alt="_images/29fcd3496b10fc8876219f2fbbf5f17dd7cad0d5166ed6f23492b506d257a574.png" src="_images/29fcd3496b10fc8876219f2fbbf5f17dd7cad0d5166ed6f23492b506d257a574.png" />
</div>
</div>
<p>The advantages of GridSearchCV is that it takes only a few lines of code to implement, and it gives you a lot of data on the fits once you train the algorithm.  However, GridSearchCV has very long run times compared to the previous two methods, and, like the for loop method, it only searches over a given range of parameters and does not consider other values (that could be better fits).</p>
</section>
<section id="randomizedsearchcv">
<h4>RandomizedSearchCV<a class="headerlink" href="#randomizedsearchcv" title="Permalink to this headline">#</a></h4>
<p>RandomizedSearchCV takes a different approach. Instead of performing a brute force search over a given range of parameters, it randomly draws a given number of points from a distribution. In the below example, we are sampling points from a uniform distribution, but there are other options as well avalible <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/stats.html">here</a> under the “Continuous distributions” heading. We choose the number of randomly sampled points by setting the <code class="docutils literal notranslate"><span class="pre">n_iter</span></code> argument.  Below, we are setting it to 5,000 points so that it is comparing the same number of values as the previous two methods so the run times are comparable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">RandomizedSearchCV</span>
<span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">uniform</span>

<span class="n">distributions</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span><span class="n">uniform</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">4</span><span class="p">)}</span>

<span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">()</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">ridge</span><span class="p">,</span> <span class="n">distributions</span><span class="p">,</span>\
                                   <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;alpha&#39;: 0.13297727996882802} -0.29149683070373633
CPU times: user 13.3 s, sys: 162 ms, total: 13.5 s
Wall time: 13.1 s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">])</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">err</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;CHOSEN ALPHA:&#39;</span><span class="p">,</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;alpha&#39;</span><span class="p">])</span>

<span class="n">X_test_plot</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_plot</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_test</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;True&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_plot</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_pred</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE: 0.3014491645647441
CHOSEN ALPHA: 0.13297727996882802
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x13fa2d1f0&gt;
</pre></div>
</div>
<img alt="_images/29fcd3496b10fc8876219f2fbbf5f17dd7cad0d5166ed6f23492b506d257a574.png" src="_images/29fcd3496b10fc8876219f2fbbf5f17dd7cad0d5166ed6f23492b506d257a574.png" />
</div>
</div>
<p>The advantages of the RandomizedSearchCV method are the same as GridSearchCV (only a few line implementation and it gives a lot of data once it is trained). Additionally, unlike the for loop method and GridSearchCV, since this method draws points from a distribution, it is not constrained to a given range of values the user passes. However, it also has long run times.  This can be changed by lowering the <code class="docutils literal notranslate"><span class="pre">n_iter</span></code> value, but smaller values mean less values are tested.  Additionally, like the previous methods, it only searches a finite number of parameter combinations.</p>
</section>
<section id="bayesian-ridge-regression">
<h4>Bayesian Ridge Regression<a class="headerlink" href="#bayesian-ridge-regression" title="Permalink to this headline">#</a></h4>
<p>Bayesian ridge regression belongs to a classification of  machine learning algorithms called <a class="reference external" href="https://machinelearningmastery.com/bayes-theorem-for-machine-learning/">Bayesian machine learning</a>. Instead of using “normal” stastics to fit the models Bayesian machine learning makes use of <a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_statistics">Bayesian statistics</a>, which is based on <a class="reference external" href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes Theorem</a>.</p>
<p>Bayesian ridge regression uses Bayesian statistics to find the value of <span class="math notranslate nohighlight">\(\alpha\)</span> that is statistically likely to result in the best model. This fixes the problems of the previous models where we could only search a finite number of values. We will not go into detail about how the Bayesian ridge regression algorithm determines the best value of <span class="math notranslate nohighlight">\(\alpha\)</span>, but if you are interested, the following links are a good starting place:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/linear_model.html#bayesian-regression">Scikit-Learn’s website</a></p></li>
<li><p><a class="reference external" href="https://buildingblock.ai/bayesian-ridge-regression">Building Block AI</a></p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/how-to-build-a-bayesian-ridge-regression-model-with-full-hyperparameter-integration-f4ac2bdaf329">Towards Data Science</a></p></li>
</ul>
<p>You will get different results when performing Bayesian ridge regresion compared to linear regression or ridge regression, and the results may not be as accurate as a throughly tuned ridge regression model. However, Bayesian ridge regression is much more robust than regular ridge regression and performs it hyperparameter tuning automatically.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">BayesianRidge</span>

<span class="n">bayesian_ridge</span> <span class="o">=</span> <span class="n">BayesianRidge</span><span class="p">()</span>
<span class="n">bayesian_ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">bayesian_ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">bayesian_ridge</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>3.5079737637644315
CPU times: user 7.04 ms, sys: 1 ms, total: 8.05 ms
Wall time: 2.84 ms
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge</span> <span class="o">=</span> <span class="n">BayesianRidge</span><span class="p">()</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">err</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;CHOSEN ALPHA:&#39;</span><span class="p">,</span> <span class="n">ridge</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>

<span class="n">X_test_plot</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_plot</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_test</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;True&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test_plot</span><span class="p">[:,</span><span class="mi">1</span><span class="p">],</span><span class="n">y_pred</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Predicted&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE: 0.29993182718830796
CHOSEN ALPHA: 3.5079737637644315
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x13feb64c0&gt;
</pre></div>
</div>
<img alt="_images/6e0d3b30e6d79e965abca4aa8c50f55fbf5dd3155af1ed2f5722cf079c0604c8.png" src="_images/6e0d3b30e6d79e965abca4aa8c50f55fbf5dd3155af1ed2f5722cf079c0604c8.png" />
</div>
</div>
<p>Bayesian ridge regression has very short run times and you can be sure that you have found the statistically best value of <span class="math notranslate nohighlight">\(\alpha\)</span>, instead of just the best value in a given finite range. It also only takes a couple of lines of code to implement. Unfortunately, Bayesian ridge regression only works for ridge regression (unlike the previous models which can be extended to other machine learning algorithms), but there are Bayesian machine learning implementations of most common machine learning algorithhms. <a class="reference external" href="https://scikit-learn.org/stable/modules/gaussian_process.html">Gaussian processes</a> can be considered the Bayesian implementation of several nonlinear models, such as kernel ridge regression we will look at later in these notes. <a class="reference external" href="https://arxiv.org/pdf/2007.06823.pdf">Bayesian Neural Networks</a> are the Bayesian implementation of neural networks.</p>
</section>
</section>
</section>
<section id="part-ii-feature-engineering">
<h2>Part II: Feature Engineering<a class="headerlink" href="#part-ii-feature-engineering" title="Permalink to this headline">#</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Feature_engineering">Feature engineering</a> is the process of eliminating or altering the inputs to a model in order to improve a model’s predictive performance. We have already seen several examples of feature engineering the this class so far. Using a design matrix to modify the inputs is an example of feature engineering and so is scaling the data set using standard scaler since these alter the values of the inputs. We have also looked into removing some columns of a data frame before training our models, leaving only the most relevant features.  This is also feature engineering.</p>
<p>Below we will attempt to improve the Bayesian ridge regression results applied to the California housing data set using various types of feature engineering.  First, we will import the data set as a Pandas DataFrame and then we will use the <code class="docutils literal notranslate"><span class="pre">sample</span></code> function to randomly select 1,000 points to use in our test to make the analysis faster.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="n">housing</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>

<span class="n">housing_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">housing</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">housing</span><span class="o">.</span><span class="n">feature_names</span><span class="p">)</span>

<span class="n">housing_data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">housing</span><span class="o">.</span><span class="n">target</span>

<span class="n">housing_data</span> <span class="o">=</span> <span class="n">housing_data</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The first thing we can do is create a pairplot using Seaborn to look for correlations between the data. We should pay particular attention to the bottom row of the output, as those are the correlations we are trying to help the model run (the target column as the dependent variable and the other columns as the independent variables).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">housing_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight
  self._figure.tight_layout(*args, **kwargs)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.PairGrid at 0x13fd145e0&gt;
</pre></div>
</div>
<img alt="_images/17ecceba450a8fe8ae61905455c43fb65269abaac42afc9d2b1326a565bbd611.png" src="_images/17ecceba450a8fe8ae61905455c43fb65269abaac42afc9d2b1326a565bbd611.png" />
</div>
</div>
<p>It can be hard to draw conclusions from this data set since in many cases there is no clear pattern or where then maybe a pattern it is not a linear pattern. Another way we can determine patterns in the data is to create a <strong>correlation matrix</strong>. A correlation matrix displays the correlation score (the square root of the R2-score) for all possible combinations of variables. The closer the correlation score is to <span class="math notranslate nohighlight">\(\pm 1\)</span>, the more linear the plot of those two variables will be (which is important when we are using linear models).  We can create a correlation matrix using the following code.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">correlation_matrix</span> <span class="o">=</span> <span class="n">housing_data</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">correlation_matrix</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>MedInc</th>
      <td>1.000000</td>
      <td>-0.185583</td>
      <td>0.391977</td>
      <td>-0.081648</td>
      <td>-0.004763</td>
      <td>-0.005193</td>
      <td>-0.063743</td>
      <td>-0.017708</td>
      <td>0.651168</td>
    </tr>
    <tr>
      <th>HouseAge</th>
      <td>-0.185583</td>
      <td>1.000000</td>
      <td>-0.188034</td>
      <td>-0.077416</td>
      <td>-0.237708</td>
      <td>-0.009751</td>
      <td>0.001094</td>
      <td>-0.109600</td>
      <td>0.079301</td>
    </tr>
    <tr>
      <th>AveRooms</th>
      <td>0.391977</td>
      <td>-0.188034</td>
      <td>1.000000</td>
      <td>0.777403</td>
      <td>-0.052323</td>
      <td>0.104751</td>
      <td>0.124733</td>
      <td>-0.009820</td>
      <td>0.133150</td>
    </tr>
    <tr>
      <th>AveBedrms</th>
      <td>-0.081648</td>
      <td>-0.077416</td>
      <td>0.777403</td>
      <td>1.000000</td>
      <td>-0.063132</td>
      <td>0.002877</td>
      <td>0.099226</td>
      <td>0.025022</td>
      <td>-0.082664</td>
    </tr>
    <tr>
      <th>Population</th>
      <td>-0.004763</td>
      <td>-0.237708</td>
      <td>-0.052323</td>
      <td>-0.063132</td>
      <td>1.000000</td>
      <td>0.224197</td>
      <td>-0.109404</td>
      <td>0.100037</td>
      <td>-0.017540</td>
    </tr>
    <tr>
      <th>AveOccup</th>
      <td>-0.005193</td>
      <td>-0.009751</td>
      <td>0.104751</td>
      <td>0.002877</td>
      <td>0.224197</td>
      <td>1.000000</td>
      <td>-0.155917</td>
      <td>0.171381</td>
      <td>-0.237492</td>
    </tr>
    <tr>
      <th>Latitude</th>
      <td>-0.063743</td>
      <td>0.001094</td>
      <td>0.124733</td>
      <td>0.099226</td>
      <td>-0.109404</td>
      <td>-0.155917</td>
      <td>1.000000</td>
      <td>-0.922275</td>
      <td>-0.097095</td>
    </tr>
    <tr>
      <th>Longitude</th>
      <td>-0.017708</td>
      <td>-0.109600</td>
      <td>-0.009820</td>
      <td>0.025022</td>
      <td>0.100037</td>
      <td>0.171381</td>
      <td>-0.922275</td>
      <td>1.000000</td>
      <td>-0.098079</td>
    </tr>
    <tr>
      <th>target</th>
      <td>0.651168</td>
      <td>0.079301</td>
      <td>0.133150</td>
      <td>-0.082664</td>
      <td>-0.017540</td>
      <td>-0.237492</td>
      <td>-0.097095</td>
      <td>-0.098079</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>We can also display the correlation matrix with <code class="docutils literal notranslate"><span class="pre">matshow</span></code> like we did with the confusion matrices. From this matrix we can see that the features most correlated with our housing prices is the median income in the area, and the second most correlated features are the average rooms in the house and the average occupancy of the house.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">matshow</span><span class="p">(</span><span class="n">correlation_matrix</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">colorbar</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.colorbar.Colorbar at 0x283537ca0&gt;
</pre></div>
</div>
<img alt="_images/d25e1de2c12436b8ee6d9965091f5063fc035b1acbeba6ed4924dbeb524a46a1.png" src="_images/d25e1de2c12436b8ee6d9965091f5063fc035b1acbeba6ed4924dbeb524a46a1.png" />
</div>
</div>
<p>From here we can train a chosen model (Bayesian ridge regression in this case) on the data without performing any feature engineering to get a base line score.  Then we will see if by performing any of the feature engineering methods if we can improve the score.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">housing_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">housing_data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge</span> <span class="o">=</span> <span class="n">BayesianRidge</span><span class="p">()</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">err</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;CHOSEN ALPHA:&#39;</span><span class="p">,</span> <span class="n">ridge</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE: 0.3499180694071488
CHOSEN ALPHA: 2.0736881062171597
</pre></div>
</div>
</div>
</div>
<p>If we look at the pairplot graphs above, many of the graphs seem to have outliers (points whose values does not fit in with the general pattern). Generally it is acceptable to remove outliers from a training set as it is assumed outliers are difficult if not impossible to predict. However, we cannot just toss out points randomly because they do not fit our model.  Instead we need to perform statistical test to remove the outliers. The below cell does that using the <a class="reference external" href="https://en.wikipedia.org/wiki/Standard_score">Z-score test</a>. There are also other statistical test and models you can use to identify and remove outliers.  The below code removes the outliers and then reproduces the pairplot to make sure they have (mostly) been removed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Remove outliers from the data set and make the pairplot again</span>

<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>

<span class="n">housing_data_no_outliers</span> <span class="o">=</span> <span class="n">housing_data</span><span class="p">[(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">stats</span><span class="o">.</span><span class="n">zscore</span><span class="p">(</span><span class="n">housing_data</span><span class="p">))</span> <span class="o">&lt;</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)]</span>

<span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">housing_data_no_outliers</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/seaborn/axisgrid.py:118: UserWarning: The figure layout has changed to tight
  self._figure.tight_layout(*args, **kwargs)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;seaborn.axisgrid.PairGrid at 0x28368ed30&gt;
</pre></div>
</div>
<img alt="_images/4a07114a58505859828969f92457935e25da6140f9efcce24722f679834b36ca.png" src="_images/4a07114a58505859828969f92457935e25da6140f9efcce24722f679834b36ca.png" />
</div>
</div>
<p>We can also recreate our correlation matrix to see if any of the relevant correlation scores have changed.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">correlation_matrix</span> <span class="o">=</span> <span class="n">housing_data_no_outliers</span><span class="o">.</span><span class="n">corr</span><span class="p">()</span>
<span class="n">correlation_matrix</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>MedInc</th>
      <th>HouseAge</th>
      <th>AveRooms</th>
      <th>AveBedrms</th>
      <th>Population</th>
      <th>AveOccup</th>
      <th>Latitude</th>
      <th>Longitude</th>
      <th>target</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>MedInc</th>
      <td>1.000000</td>
      <td>-0.184528</td>
      <td>0.664298</td>
      <td>-0.218128</td>
      <td>-0.017218</td>
      <td>-0.013118</td>
      <td>-0.072517</td>
      <td>-0.001518</td>
      <td>0.595204</td>
    </tr>
    <tr>
      <th>HouseAge</th>
      <td>-0.184528</td>
      <td>1.000000</td>
      <td>-0.231715</td>
      <td>-0.074405</td>
      <td>-0.302704</td>
      <td>-0.012765</td>
      <td>0.005555</td>
      <td>-0.107843</td>
      <td>0.100434</td>
    </tr>
    <tr>
      <th>AveRooms</th>
      <td>0.664298</td>
      <td>-0.231715</td>
      <td>1.000000</td>
      <td>0.227421</td>
      <td>-0.051267</td>
      <td>0.052402</td>
      <td>0.096328</td>
      <td>-0.044082</td>
      <td>0.229946</td>
    </tr>
    <tr>
      <th>AveBedrms</th>
      <td>-0.218128</td>
      <td>-0.074405</td>
      <td>0.227421</td>
      <td>1.000000</td>
      <td>0.037191</td>
      <td>-0.119901</td>
      <td>0.030745</td>
      <td>0.018236</td>
      <td>-0.060258</td>
    </tr>
    <tr>
      <th>Population</th>
      <td>-0.017218</td>
      <td>-0.302704</td>
      <td>-0.051267</td>
      <td>0.037191</td>
      <td>1.000000</td>
      <td>0.192327</td>
      <td>-0.103728</td>
      <td>0.097139</td>
      <td>-0.013426</td>
    </tr>
    <tr>
      <th>AveOccup</th>
      <td>-0.013118</td>
      <td>-0.012765</td>
      <td>0.052402</td>
      <td>-0.119901</td>
      <td>0.192327</td>
      <td>1.000000</td>
      <td>-0.169848</td>
      <td>0.186525</td>
      <td>-0.287374</td>
    </tr>
    <tr>
      <th>Latitude</th>
      <td>-0.072517</td>
      <td>0.005555</td>
      <td>0.096328</td>
      <td>0.030745</td>
      <td>-0.103728</td>
      <td>-0.169848</td>
      <td>1.000000</td>
      <td>-0.929701</td>
      <td>-0.097268</td>
    </tr>
    <tr>
      <th>Longitude</th>
      <td>-0.001518</td>
      <td>-0.107843</td>
      <td>-0.044082</td>
      <td>0.018236</td>
      <td>0.097139</td>
      <td>0.186525</td>
      <td>-0.929701</td>
      <td>1.000000</td>
      <td>-0.093685</td>
    </tr>
    <tr>
      <th>target</th>
      <td>0.595204</td>
      <td>0.100434</td>
      <td>0.229946</td>
      <td>-0.060258</td>
      <td>-0.013426</td>
      <td>-0.287374</td>
      <td>-0.097268</td>
      <td>-0.093685</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<p>Now we will perform the machine learning again on this outlier-less data set and see how the MSE score improved.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">housing_data_no_outliers</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">housing_data_no_outliers</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge</span> <span class="o">=</span> <span class="n">BayesianRidge</span><span class="p">()</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">err</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;CHOSEN ALPHA:&#39;</span><span class="p">,</span> <span class="n">ridge</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE: 0.3920892135021881
CHOSEN ALPHA: 2.260218628799585
</pre></div>
</div>
</div>
</div>
<p>It appears that the MSE is lower (so the model is a better fit) when we remove the outliers, though the exact results depend on the randomless of the <code class="docutils literal notranslate"><span class="pre">split</span></code> and <code class="docutils literal notranslate"><span class="pre">train_test_split</span></code> functions. Now let’s reset our data test the effects of scaling the data on the model MSE.  We will revert to the data set with the outliers because we only want to measure once change at a time.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">housing_data</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">housing_data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge</span> <span class="o">=</span> <span class="n">BayesianRidge</span><span class="p">()</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">err</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;MSE:&quot;</span><span class="p">,</span> <span class="n">err</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;CHOSEN ALPHA:&#39;</span><span class="p">,</span> <span class="n">ridge</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE: 0.5599649212138086
CHOSEN ALPHA: 2.327648364441302
</pre></div>
</div>
</div>
</div>
<p>Though the exact results depend on the random methods, the average MSE (if run several times) from the scaled data should be less than the unscaled MSE.  Next we are going to attempt feature engineering by removing some of the features to hopefully give the model a more clear picture of the relationships in the data set. Since the median income feature had the highest correlation score with the target data, let’s try just using that one feature as our model input.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">housing_data</span><span class="p">[</span><span class="s1">&#39;MedInc&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">housing_data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">])</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge</span> <span class="o">=</span> <span class="n">BayesianRidge</span><span class="p">()</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">err</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RMSE:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">err</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;CHOSEN ALPHA:&#39;</span><span class="p">,</span> <span class="n">ridge</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RMSE: 0.7943551419306852
CHOSEN ALPHA: 1.3844282063853675
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x28a1e4e50&gt;
</pre></div>
</div>
<img alt="_images/dc44e6aab5acc988e39ac323231ed37735df48ae998829797915799927dda789.png" src="_images/dc44e6aab5acc988e39ac323231ed37735df48ae998829797915799927dda789.png" />
</div>
</div>
<p>Unfortunately, with just one feature, even if it is highly correlated, the best fit we can get is a straight line through the data. Instead of just one feature, let’s instead take the three features with the highest correlation scores and use those as the inputs to our model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">housing_data</span><span class="p">[[</span><span class="s1">&#39;MedInc&#39;</span><span class="p">,</span><span class="s1">&#39;AveRooms&#39;</span><span class="p">,</span><span class="s1">&#39;AveOccup&#39;</span><span class="p">]])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">housing_data</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">])</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ridge</span> <span class="o">=</span> <span class="n">BayesianRidge</span><span class="p">()</span>
<span class="n">ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">err</span> <span class="o">=</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RMSE:&quot;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">err</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;CHOSEN ALPHA:&#39;</span><span class="p">,</span> <span class="n">ridge</span><span class="o">.</span><span class="n">alpha_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RMSE: 0.8080517079616615
CHOSEN ALPHA: 1.615792053436879
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_test</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:,</span><span class="mi">0</span><span class="p">],</span><span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PathCollection at 0x28b5008b0&gt;
</pre></div>
</div>
<img alt="_images/ac8064b005d40325ffbb4767ca96cfe868a478962a407661881fb7022f9fc17e.png" src="_images/ac8064b005d40325ffbb4767ca96cfe868a478962a407661881fb7022f9fc17e.png" />
</div>
</div>
<p>Even with the three most correlated features, the average MSE error is still higher than that of the model that included all of the data.  So it appears, that at least of this data set, that reducing the number of features does not actually improve the model performance.</p>
<p>We have explored a few different methods of feature engineering (one at a time) on this data set.  However, in practice, you will likely attempt these three methods (and others) in various combinations to create a good model.  Part of being a good machine learning engineer is being good at formatting data in exactly the way a model needs to best find the patterns.  We will cover one more type of feature engineering next week (dimensionality reduction) and you can find plenty of examples of how other people format their data sets online (for example on <a class="reference external" href="http://kaggle.com">kaggle.com</a>).</p>
<p>We have done our best to model the California housing data set with ridge regression, performing both hyperparameter tuning and feature engineering to attempt to improve the results. However, while we have gotten some okay answers, we have not gotten a really good model yet. This could be because we are using a linear model (ridge regression) but our data set has nonlinear patterns.  In this case, we need to start using nonlinear machine learning models which can more throughly pick up on the nonlinear patterns in the data set.</p>
</section>
<section id="part-3-non-linear-models">
<h2>Part 3: Non-linear Models<a class="headerlink" href="#part-3-non-linear-models" title="Permalink to this headline">#</a></h2>
<p>All of the models we have studies so far in this course have been linear models, meaning that they are capable of modeling linear relationships in the data.  We can use design matrices to add some nonlinearity into the models but this only helps so much.  Therefore, we need to start looking machine learning models that have nonlinearity encoded into them. Examples of nonlinear models include kernel ridge regression (this week), support vector machines (not covered in this course), and neural networks (Weeks 7+).</p>
<section id="kernel-ridge-regression">
<h3>Kernel Ridge Regression<a class="headerlink" href="#kernel-ridge-regression" title="Permalink to this headline">#</a></h3>
<p>Kernel ridge regression (KRR) is essentially ridge regression, but it modifies its inputs by passing them through a nonlinear <strong>kernel function</strong>. This kernel function is what adds the nonlinearity into the model. A complete list of Scikit-Learn kernels are found <a class="reference external" href="https://scikit-learn.org/stable/modules/metrics.html">here</a>, but a few of the common ones are also listed below.</p>
<ul class="simple">
<li><p>Linear: <span class="math notranslate nohighlight">\(k(x,y) = x^Ty\)</span></p></li>
<li><p>Polynomial: k(x,y) = <span class="math notranslate nohighlight">\((\gamma x^Ty+c_0)^d\)</span></p></li>
<li><p>Sigmoid: <span class="math notranslate nohighlight">\(k(x,y) = tanh(\gamma x^Ty+c_0)\)</span></p></li>
<li><p>Radial Basis Function (RBF): <span class="math notranslate nohighlight">\(k(x,y) = exp(-\gamma||x-y||_2)\)</span></p></li>
</ul>
<p>Note that in the kernel functions, <span class="math notranslate nohighlight">\(\gamma\)</span>, <span class="math notranslate nohighlight">\(d\)</span>, and <span class="math notranslate nohighlight">\(c_0\)</span> are hyperparameters.  The choice of kernel function itself can also be considered a hyperparameter. Thus without even considering the loss function we already have 1-4 hyperparameters per model.  This is why we covered hyperparameter tuning in depth earlier in these notes, our models are going to acquire an increasing number of hyperparameters as this class advances, making effecient and thorough hyperparameter important.</p>
<section id="kernel-ridge-regression-equations">
<h4>Kernel Ridge Regression Equations<a class="headerlink" href="#kernel-ridge-regression-equations" title="Permalink to this headline">#</a></h4>
<p>The kernel ridge regression equations are going to be similar to ridge regression, but with the addition of the the kernel function. Thus, the outputs of a KRR algorithm can be models as <span class="math notranslate nohighlight">\(\hat{y}(x) = \sum_{i=1}^m\theta_ik(x_i,x)\)</span>, where k(x,y) is the kernel function and <span class="math notranslate nohighlight">\(x_i\)</span> is an iteration through the m training points given to the model. The loss function is exactly the same as for ridge regression, being the MSE function with the L2-norm of the weights:  <span class="math notranslate nohighlight">\(J(\theta) = MSE(y,\hat{y}) + \frac{\alpha}{2}\sum_{i=1}^n\theta_i^2\)</span>. This means that in addition to the 1-4 hyperparameters that come from the kernel function, we still have the <span class="math notranslate nohighlight">\(\alpha\)</span> hyperparameter.  We can still write a closed form solution for our optimized weights (the last model we can do this for).  Here we need to introduct the kernel matrix <span class="math notranslate nohighlight">\(K_{ij}=k(x_i,x_j)\)</span> where both <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(x_j\)</span> iterate through the training data. Then the optimized weights for a kernel ridge ergression model can be written as <span class="math notranslate nohighlight">\(\theta = (\textbf{K}-\alpha\textbf{I})y\)</span>.</p>
</section>
</section>
<section id="hyperparameter-tuning-with-many-hyperparameters">
<h3>Hyperparameter Tuning with Many Hyperparameters<a class="headerlink" href="#hyperparameter-tuning-with-many-hyperparameters" title="Permalink to this headline">#</a></h3>
<p>In this section of the notes, we will attempt to perform hyperparameter tuning on the kernel ridge regression applied to the California housing data set. This will be a more complicated tuning because we have 5 hyperparameters to search over (<span class="math notranslate nohighlight">\(\alpha\)</span>, the kernel function, <span class="math notranslate nohighlight">\(\gamma\)</span>, <span class="math notranslate nohighlight">\(d\)</span>, <span class="math notranslate nohighlight">\(c_0\)</span>).  In this section we will be using the RandomizedSearchCV to perform the search, but the for loops or the GridSearchCV methods could be used as well. We will be using only 500 points for this hyperparameter tuning instead of 1,000 since this tuning is much more time intensive.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">500</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">500</span><span class="p">]</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now let’s perform the tuning to find the best set of parameters.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.kernel_ridge</span> <span class="kn">import</span> <span class="n">KernelRidge</span>

<span class="n">distributions</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span><span class="n">uniform</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span> <span class="s1">&#39;kernel&#39;</span><span class="p">:[</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> \
                                                            <span class="s1">&#39;polynomial&#39;</span><span class="p">,</span> <span class="s1">&#39;rbf&#39;</span><span class="p">,</span> \
                                                            <span class="s1">&#39;sigmoid&#39;</span><span class="p">,</span> <span class="s1">&#39;laplacian&#39;</span><span class="p">],</span> \
                 <span class="s1">&#39;gamma&#39;</span><span class="p">:</span><span class="n">uniform</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">4</span><span class="p">),</span>\
                <span class="s1">&#39;degree&#39;</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">),</span> <span class="s1">&#39;coef0&#39;</span><span class="p">:</span><span class="n">uniform</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">4</span><span class="p">)}</span>

<span class="n">krr</span> <span class="o">=</span> <span class="n">KernelRidge</span><span class="p">()</span>

<span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">krr</span><span class="p">,</span> <span class="n">distributions</span><span class="p">,</span>\
                                   <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random_search</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:249: LinAlgWarning: Ill-conditioned matrix (rcond=1.62527e-17): result may not be accurate.
  dual_coef = linalg.solve(K, y, assume_a=&quot;pos&quot;, overwrite_a=False)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:249: LinAlgWarning: Ill-conditioned matrix (rcond=1.0463e-17): result may not be accurate.
  dual_coef = linalg.solve(K, y, assume_a=&quot;pos&quot;, overwrite_a=False)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:249: LinAlgWarning: Ill-conditioned matrix (rcond=1.12319e-17): result may not be accurate.
  dual_coef = linalg.solve(K, y, assume_a=&quot;pos&quot;, overwrite_a=False)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:249: LinAlgWarning: Ill-conditioned matrix (rcond=1.03857e-17): result may not be accurate.
  dual_coef = linalg.solve(K, y, assume_a=&quot;pos&quot;, overwrite_a=False)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/Users/juliehartley/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:251: UserWarning: Singular matrix in solving dual problem. Using least-squares solution instead.
  warnings.warn(
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span> <span class="p">[</span><span class="mi">35</span><span class="p">],</span> <span class="n">line</span> <span class="mi">13</span>
<span class="g g-Whitespace">      </span><span class="mi">9</span> <span class="n">krr</span> <span class="o">=</span> <span class="n">KernelRidge</span><span class="p">()</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span> <span class="n">random_search</span> <span class="o">=</span> <span class="n">RandomizedSearchCV</span><span class="p">(</span><span class="n">krr</span><span class="p">,</span> <span class="n">distributions</span><span class="p">,</span>\
<span class="g g-Whitespace">     </span><span class="mi">12</span>                                    <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;neg_mean_squared_error&#39;</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">13</span> <span class="n">random_search</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span> <span class="nb">print</span><span class="p">(</span><span class="n">random_search</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> <span class="nb">print</span><span class="p">(</span><span class="n">random_search</span><span class="o">.</span><span class="n">best_params_</span><span class="p">,</span> <span class="n">random_search</span><span class="o">.</span><span class="n">best_score_</span><span class="p">)</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:875,</span> in <span class="ni">BaseSearchCV.fit</span><span class="nt">(self, X, y, groups, **fit_params)</span>
<span class="g g-Whitespace">    </span><span class="mi">869</span>     <span class="n">results</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_format_results</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">870</span>         <span class="n">all_candidate_params</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">,</span> <span class="n">all_out</span><span class="p">,</span> <span class="n">all_more_results</span>
<span class="g g-Whitespace">    </span><span class="mi">871</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">873</span>     <span class="k">return</span> <span class="n">results</span>
<span class="ne">--&gt; </span><span class="mi">875</span> <span class="bp">self</span><span class="o">.</span><span class="n">_run_search</span><span class="p">(</span><span class="n">evaluate_candidates</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">877</span> <span class="c1"># multimetric is determined here because in the case of a callable</span>
<span class="g g-Whitespace">    </span><span class="mi">878</span> <span class="c1"># self.scoring the return type is only known after calling</span>
<span class="g g-Whitespace">    </span><span class="mi">879</span> <span class="n">first_test_score</span> <span class="o">=</span> <span class="n">all_out</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;test_scores&quot;</span><span class="p">]</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:1753,</span> in <span class="ni">RandomizedSearchCV._run_search</span><span class="nt">(self, evaluate_candidates)</span>
<span class="g g-Whitespace">   </span><span class="mi">1751</span> <span class="k">def</span> <span class="nf">_run_search</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">evaluate_candidates</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">1752</span>     <span class="sd">&quot;&quot;&quot;Search n_iter candidates from param_distributions&quot;&quot;&quot;</span>
<span class="ne">-&gt; </span><span class="mi">1753</span>     <span class="n">evaluate_candidates</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1754</span>         <span class="n">ParameterSampler</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1755</span>             <span class="bp">self</span><span class="o">.</span><span class="n">param_distributions</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_iter</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span>
<span class="g g-Whitespace">   </span><span class="mi">1756</span>         <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1757</span>     <span class="p">)</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_search.py:822,</span> in <span class="ni">BaseSearchCV.fit.&lt;locals&gt;.evaluate_candidates</span><span class="nt">(candidate_params, cv, more_results)</span>
<span class="g g-Whitespace">    </span><span class="mi">814</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">verbose</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">815</span>     <span class="nb">print</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">816</span>         <span class="s2">&quot;Fitting </span><span class="si">{0}</span><span class="s2"> folds for each of </span><span class="si">{1}</span><span class="s2"> candidates,&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">817</span>         <span class="s2">&quot; totalling </span><span class="si">{2}</span><span class="s2"> fits&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">818</span>             <span class="n">n_splits</span><span class="p">,</span> <span class="n">n_candidates</span><span class="p">,</span> <span class="n">n_candidates</span> <span class="o">*</span> <span class="n">n_splits</span>
<span class="g g-Whitespace">    </span><span class="mi">819</span>         <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">820</span>     <span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">822</span> <span class="n">out</span> <span class="o">=</span> <span class="n">parallel</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">823</span>     <span class="n">delayed</span><span class="p">(</span><span class="n">_fit_and_score</span><span class="p">)(</span>
<span class="g g-Whitespace">    </span><span class="mi">824</span>         <span class="n">clone</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">825</span>         <span class="n">X</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">826</span>         <span class="n">y</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">827</span>         <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">828</span>         <span class="n">test</span><span class="o">=</span><span class="n">test</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">829</span>         <span class="n">parameters</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">830</span>         <span class="n">split_progress</span><span class="o">=</span><span class="p">(</span><span class="n">split_idx</span><span class="p">,</span> <span class="n">n_splits</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">831</span>         <span class="n">candidate_progress</span><span class="o">=</span><span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">n_candidates</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">832</span>         <span class="o">**</span><span class="n">fit_and_score_kwargs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">833</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">834</span>     <span class="k">for</span> <span class="p">(</span><span class="n">cand_idx</span><span class="p">,</span> <span class="n">parameters</span><span class="p">),</span> <span class="p">(</span><span class="n">split_idx</span><span class="p">,</span> <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">))</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">835</span>         <span class="nb">enumerate</span><span class="p">(</span><span class="n">candidate_params</span><span class="p">),</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">cv</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">groups</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">836</span>     <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">837</span> <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">839</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">840</span>     <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">841</span>         <span class="s2">&quot;No fits were performed. &quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">842</span>         <span class="s2">&quot;Was the CV iterator empty? &quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">843</span>         <span class="s2">&quot;Were there no candidates?&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">844</span>     <span class="p">)</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:1088,</span> in <span class="ni">Parallel.__call__</span><span class="nt">(self, iterable)</span>
<span class="g g-Whitespace">   </span><span class="mi">1085</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dispatch_one_batch</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">1086</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_iterating</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_original_iterator</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
<span class="ne">-&gt; </span><span class="mi">1088</span> <span class="k">while</span> <span class="bp">self</span><span class="o">.</span><span class="n">dispatch_one_batch</span><span class="p">(</span><span class="n">iterator</span><span class="p">):</span>
<span class="g g-Whitespace">   </span><span class="mi">1089</span>     <span class="k">pass</span>
<span class="g g-Whitespace">   </span><span class="mi">1091</span> <span class="k">if</span> <span class="n">pre_dispatch</span> <span class="o">==</span> <span class="s2">&quot;all&quot;</span> <span class="ow">or</span> <span class="n">n_jobs</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1092</span>     <span class="c1"># The iterable was consumed all at once by the above for loop.</span>
<span class="g g-Whitespace">   </span><span class="mi">1093</span>     <span class="c1"># No need to wait for async callbacks to trigger to</span>
<span class="g g-Whitespace">   </span><span class="mi">1094</span>     <span class="c1"># consumption.</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:901,</span> in <span class="ni">Parallel.dispatch_one_batch</span><span class="nt">(self, iterator)</span>
<span class="g g-Whitespace">    </span><span class="mi">899</span>     <span class="k">return</span> <span class="kc">False</span>
<span class="g g-Whitespace">    </span><span class="mi">900</span> <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">901</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_dispatch</span><span class="p">(</span><span class="n">tasks</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">902</span>     <span class="k">return</span> <span class="kc">True</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:819,</span> in <span class="ni">Parallel._dispatch</span><span class="nt">(self, batch)</span>
<span class="g g-Whitespace">    </span><span class="mi">817</span> <span class="k">with</span> <span class="bp">self</span><span class="o">.</span><span class="n">_lock</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">818</span>     <span class="n">job_idx</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_jobs</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">819</span>     <span class="n">job</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_backend</span><span class="o">.</span><span class="n">apply_async</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="n">cb</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">820</span>     <span class="c1"># A job can complete so quickly than its callback is</span>
<span class="g g-Whitespace">    </span><span class="mi">821</span>     <span class="c1"># called before we get here, causing self._jobs to</span>
<span class="g g-Whitespace">    </span><span class="mi">822</span>     <span class="c1"># grow. To ensure correct results ordering, .insert is</span>
<span class="g g-Whitespace">    </span><span class="mi">823</span>     <span class="c1"># used (rather than .append) in the following line</span>
<span class="g g-Whitespace">    </span><span class="mi">824</span>     <span class="bp">self</span><span class="o">.</span><span class="n">_jobs</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="n">job_idx</span><span class="p">,</span> <span class="n">job</span><span class="p">)</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/joblib/_parallel_backends.py:208,</span> in <span class="ni">SequentialBackend.apply_async</span><span class="nt">(self, func, callback)</span>
<span class="g g-Whitespace">    </span><span class="mi">206</span> <span class="k">def</span> <span class="nf">apply_async</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">207</span>     <span class="sd">&quot;&quot;&quot;Schedule a func to be run&quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">208</span>     <span class="n">result</span> <span class="o">=</span> <span class="n">ImmediateResult</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">209</span>     <span class="k">if</span> <span class="n">callback</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">210</span>         <span class="n">callback</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/joblib/_parallel_backends.py:597,</span> in <span class="ni">ImmediateResult.__init__</span><span class="nt">(self, batch)</span>
<span class="g g-Whitespace">    </span><span class="mi">594</span> <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">595</span>     <span class="c1"># Don&#39;t delay the application, to avoid keeping the input</span>
<span class="g g-Whitespace">    </span><span class="mi">596</span>     <span class="c1"># arguments in memory</span>
<span class="ne">--&gt; </span><span class="mi">597</span>     <span class="bp">self</span><span class="o">.</span><span class="n">results</span> <span class="o">=</span> <span class="n">batch</span><span class="p">()</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:288,</span> in <span class="ni">BatchedCalls.__call__</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">284</span> <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">285</span>     <span class="c1"># Set the default nested backend to self._backend but do not set the</span>
<span class="g g-Whitespace">    </span><span class="mi">286</span>     <span class="c1"># change the default number of processes to -1</span>
<span class="g g-Whitespace">    </span><span class="mi">287</span>     <span class="k">with</span> <span class="n">parallel_backend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backend</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_jobs</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">288</span>         <span class="k">return</span> <span class="p">[</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">289</span>                 <span class="k">for</span> <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">]</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/joblib/parallel.py:288,</span> in <span class="ni">&lt;listcomp&gt;</span><span class="nt">(.0)</span>
<span class="g g-Whitespace">    </span><span class="mi">284</span> <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">285</span>     <span class="c1"># Set the default nested backend to self._backend but do not set the</span>
<span class="g g-Whitespace">    </span><span class="mi">286</span>     <span class="c1"># change the default number of processes to -1</span>
<span class="g g-Whitespace">    </span><span class="mi">287</span>     <span class="k">with</span> <span class="n">parallel_backend</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_backend</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_n_jobs</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">288</span>         <span class="k">return</span> <span class="p">[</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">289</span>                 <span class="k">for</span> <span class="n">func</span><span class="p">,</span> <span class="n">args</span><span class="p">,</span> <span class="n">kwargs</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">items</span><span class="p">]</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/fixes.py:117,</span> in <span class="ni">_FuncWrapper.__call__</span><span class="nt">(self, *args, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">115</span> <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">116</span>     <span class="k">with</span> <span class="n">config_context</span><span class="p">(</span><span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">):</span>
<span class="ne">--&gt; </span><span class="mi">117</span>         <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/sklearn/model_selection/_validation.py:686,</span> in <span class="ni">_fit_and_score</span><span class="nt">(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)</span>
<span class="g g-Whitespace">    </span><span class="mi">684</span>         <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">685</span>     <span class="k">else</span><span class="p">:</span>
<span class="ne">--&gt; </span><span class="mi">686</span>         <span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">688</span> <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">689</span>     <span class="c1"># Note fit time as time until error</span>
<span class="g g-Whitespace">    </span><span class="mi">690</span>     <span class="n">fit_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">start_time</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/sklearn/kernel_ridge.py:195,</span> in <span class="ni">KernelRidge.fit</span><span class="nt">(self, X, y, sample_weight)</span>
<span class="g g-Whitespace">    </span><span class="mi">192</span>     <span class="n">ravel</span> <span class="o">=</span> <span class="kc">True</span>
<span class="g g-Whitespace">    </span><span class="mi">194</span> <span class="n">copy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">==</span> <span class="s2">&quot;precomputed&quot;</span>
<span class="ne">--&gt; </span><span class="mi">195</span> <span class="bp">self</span><span class="o">.</span><span class="n">dual_coef_</span> <span class="o">=</span> <span class="n">_solve_cholesky_kernel</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">copy</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">196</span> <span class="k">if</span> <span class="n">ravel</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">197</span>     <span class="bp">self</span><span class="o">.</span><span class="n">dual_coef_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dual_coef_</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/sklearn/linear_model/_ridge.py:249,</span> in <span class="ni">_solve_cholesky_kernel</span><span class="nt">(K, y, alpha, sample_weight, copy)</span>
<span class="g g-Whitespace">    </span><span class="mi">243</span> <span class="n">K</span><span class="o">.</span><span class="n">flat</span><span class="p">[::</span> <span class="n">n_samples</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">alpha</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="g g-Whitespace">    </span><span class="mi">245</span> <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">246</span>     <span class="c1"># Note: we must use overwrite_a=False in order to be able to</span>
<span class="g g-Whitespace">    </span><span class="mi">247</span>     <span class="c1">#       use the fall-back solution below in case a LinAlgError</span>
<span class="g g-Whitespace">    </span><span class="mi">248</span>     <span class="c1">#       is raised</span>
<span class="ne">--&gt; </span><span class="mi">249</span>     <span class="n">dual_coef</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">assume_a</span><span class="o">=</span><span class="s2">&quot;pos&quot;</span><span class="p">,</span> <span class="n">overwrite_a</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">250</span> <span class="k">except</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">LinAlgError</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">251</span>     <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">252</span>         <span class="s2">&quot;Singular matrix in solving dual problem. Using &quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">253</span>         <span class="s2">&quot;least-squares solution instead.&quot;</span>
<span class="g g-Whitespace">    </span><span class="mi">254</span>     <span class="p">)</span>

<span class="nn">File ~/Library/Python/3.9/lib/python/site-packages/scipy/linalg/_basic.py:251,</span> in <span class="ni">solve</span><span class="nt">(a, b, sym_pos, lower, overwrite_a, overwrite_b, check_finite, assume_a, transposed)</span>
<span class="g g-Whitespace">    </span><span class="mi">247</span> <span class="c1"># Positive definite case &#39;posv&#39;</span>
<span class="g g-Whitespace">    </span><span class="mi">248</span> <span class="k">else</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">249</span>     <span class="n">pocon</span><span class="p">,</span> <span class="n">posv</span> <span class="o">=</span> <span class="n">get_lapack_funcs</span><span class="p">((</span><span class="s1">&#39;pocon&#39;</span><span class="p">,</span> <span class="s1">&#39;posv&#39;</span><span class="p">),</span>
<span class="g g-Whitespace">    </span><span class="mi">250</span>                                    <span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">b1</span><span class="p">))</span>
<span class="ne">--&gt; </span><span class="mi">251</span>     <span class="n">lu</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">posv</span><span class="p">(</span><span class="n">a1</span><span class="p">,</span> <span class="n">b1</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">252</span>                        <span class="n">overwrite_a</span><span class="o">=</span><span class="n">overwrite_a</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">253</span>                        <span class="n">overwrite_b</span><span class="o">=</span><span class="n">overwrite_b</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">254</span>     <span class="n">_solve_check</span><span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">255</span>     <span class="n">rcond</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">pocon</span><span class="p">(</span><span class="n">lu</span><span class="p">,</span> <span class="n">anorm</span><span class="p">)</span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<p>Now let’s test out best model (taken from one run on the above code) and test it’s performance on the entire California housing data set and compare its results to performing Baysian ridge regression on the entire California housing data set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X</span><span class="p">,</span><span class="n">y</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">scaler</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span>
<span class="n">scaler</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="n">scaler</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Using the best parameters from through one run of the </span>
<span class="c1"># RandomizedSearchCV method above</span>
<span class="n">krr</span> <span class="o">=</span> <span class="n">KernelRidge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span> <span class="mf">0.35753693232459094</span><span class="p">,</span> <span class="n">coef0</span><span class="o">=</span> <span class="mf">3.2725118241858264</span><span class="p">,</span> <span class="n">degree</span><span class="o">=</span> <span class="mi">9</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span> <span class="mf">0.14696609545731532</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span> <span class="s1">&#39;laplacian&#39;</span><span class="p">)</span>
<span class="n">krr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">krr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MSE:&#39;</span><span class="p">,</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE: 0.2380564122097256
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bayesian_ridge</span> <span class="o">=</span> <span class="n">BayesianRidge</span><span class="p">()</span>
<span class="n">bayesian_ridge</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">bayesian_ridge</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;MSE:&#39;</span><span class="p">,</span> <span class="n">MSE</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>MSE: 0.5282280190419684
</pre></div>
</div>
</div>
</div>
<p>The MSE for the tuned KRR algorithm is half that as for the Bayesian ridge regression algorithm, so it does appear that a nonlinear model is the best fit for this data set and that further hyperparameter tuning and feature engineering may be able to reduce this error further.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="Week4_LectureNotes.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Using Machine Learning to Solve Classification Problems</p>
      </div>
    </a>
    <a class="right-next"
       href="Week6_LectureNotes.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Unsupervised Machine Learning: Clustering and Dimensionality Reduction</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-1-hyperparameter-tuning">Part 1: Hyperparameter Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#why-do-we-need-hyperparameter-tuning">Why do we need hyperparameter tuning?</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#methods-for-hyperparameter-tuning">Methods for Hyperparameter Tuning</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#using-default-values">Using Default Values</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#for-loop-tuning">For Loop Tuning</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#gridsearchcv-scikit-learn">GridSearchCV (Scikit-Learn)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#randomizedsearchcv">RandomizedSearchCV</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bayesian-ridge-regression">Bayesian Ridge Regression</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-ii-feature-engineering">Part II: Feature Engineering</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-3-non-linear-models">Part 3: Non-linear Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-ridge-regression">Kernel Ridge Regression</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#kernel-ridge-regression-equations">Kernel Ridge Regression Equations</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#hyperparameter-tuning-with-many-hyperparameters">Hyperparameter Tuning with Many Hyperparameters</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Dr. Julie Butler
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>